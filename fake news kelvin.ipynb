{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras import utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import  Dense, Activation, Dropout,Bidirectional, GlobalMaxPool1D,BatchNormalization, Embedding,LSTM, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "#print(np.array(sw))\n",
    "stemmer = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_puncts = {'`': \"'\", '′': \"'\", '“':'\"', '”': '\"', '‘': \"'\"}\n",
    "\n",
    "strip_chars = [',', '.', '\"', ':', ')', '(', '-', '|', ';', \"'\", '[', ']', '>', '=', '+', '\\\\', '•',  '~', '@', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "puncts = ['!', '?', '$', '&', '/', '%', '#', '*','£']\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    x = x.lower()\n",
    "    x = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\", \"url\", x)\n",
    "    for k, v in replace_puncts.items():\n",
    "        x = x.replace(k, f' {v} ')\n",
    "        \n",
    "    for punct in strip_chars:\n",
    "        x = x.replace(punct, ' ') \n",
    "    \n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "        \n",
    "    x = x.replace(\" '\", \" \")\n",
    "    x = x.replace(\"' \", \" \")\n",
    "   \n",
    "    return x\n",
    "\n",
    "def stopwords1(text):\n",
    "    '''a function for removing the stopword'''\n",
    "    # removing the stop words and lowercasing the selected words\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    # joining the list of words with space separator\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def stemming(text):    \n",
    "    '''a function which stems each word in the given text'''\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text) \n",
    "\n",
    "def lemm(text):    \n",
    "    text = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    return \" \".join(text) \n",
    "\n",
    "def text_processing(X):\n",
    "    X=X.apply(stopwords1)\n",
    "    X=X.apply(clean_text)\n",
    "    X=X.apply(stemming)\n",
    "    X=X.apply(lemm)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = pd.read_csv('Fake.csv')\n",
    "df_true = pd.read_csv('True.csv')\n",
    "df_fake['is_true'] = 0\n",
    "df_true['is_true'] = 1\n",
    "\n",
    "#fake_title = df_fake['title']\n",
    "true_title = df_true['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat([df_fake,df_true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>is_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19683</th>\n",
       "      <td>Trump Fan Violently Attacks Comedian For Harm...</td>\n",
       "      <td>Comedian John Caparulo was in the middle of a ...</td>\n",
       "      <td>News</td>\n",
       "      <td>March 1, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4796</th>\n",
       "      <td>Germany calls for restraint, dialogue in Zimbabwe</td>\n",
       "      <td>BERLIN (Reuters) - Germany called for all side...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>November 15, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27579</th>\n",
       "      <td>Socialist Utopia of Venezuela Tells People to ...</td>\n",
       "      <td>GENERALLY, VENEZUELANS DO NOT EAT RABBITS AND...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>Sep 16, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5816</th>\n",
       "      <td>Irish deputy PM resigns, averting election threat</td>\n",
       "      <td>DUBLIN (Reuters) - Ireland s scandal-hit deput...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>November 27, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43450</th>\n",
       "      <td>Suicide risk torture victim can be deported: E...</td>\n",
       "      <td>LUXEMBOURG (Reuters) - European governments ca...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 24, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "19683   Trump Fan Violently Attacks Comedian For Harm...   \n",
       "4796   Germany calls for restraint, dialogue in Zimbabwe   \n",
       "27579  Socialist Utopia of Venezuela Tells People to ...   \n",
       "5816   Irish deputy PM resigns, averting election threat   \n",
       "43450  Suicide risk torture victim can be deported: E...   \n",
       "\n",
       "                                                    text          subject  \\\n",
       "19683  Comedian John Caparulo was in the middle of a ...             News   \n",
       "4796   BERLIN (Reuters) - Germany called for all side...        worldnews   \n",
       "27579   GENERALLY, VENEZUELANS DO NOT EAT RABBITS AND...  Government News   \n",
       "5816   DUBLIN (Reuters) - Ireland s scandal-hit deput...        worldnews   \n",
       "43450  LUXEMBOURG (Reuters) - European governments ca...        worldnews   \n",
       "\n",
       "                     date  is_true  \n",
       "19683       March 1, 2017        0  \n",
       "4796   November 15, 2017         1  \n",
       "27579        Sep 16, 2017        0  \n",
       "5816   November 27, 2017         1  \n",
       "43450   October 24, 2017         1  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = shuffle(df_data).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_data['title']\n",
    "y = df_data['is_true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_x = text_processing(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_temp = y_test\n",
    "#max_words = 3000\n",
    "#tokenize = text.Tokenizer(num_words=max_words)\n",
    "#tokenize.fit_on_texts(x_train) # only fit on train\n",
    "#x_train = tokenize.texts_to_sequences(x_train)\n",
    "#x_test = tokenize.texts_to_sequences(x_test)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequences= 300\n",
    "x_train=pad_sequences(x_train,maxlen=max_sequences)\n",
    "x_test=pad_sequences(x_test,maxlen=max_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kelvin\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#LSTM DTS\n",
    "embed_size = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words,embed_size,input_length=max_sequences))\n",
    "model.add(Bidirectional((LSTM(64,return_sequences = True,recurrent_dropout=0.5))))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(48, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(y.unique()), activation=\"softmax\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kelvin\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\kelvin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 25142 samples, validate on 6286 samples\n",
      "Epoch 1/10\n",
      "25142/25142 [==============================] - 160s 6ms/step - loss: 0.1820 - accuracy: 0.9236 - val_loss: 0.3816 - val_accuracy: 0.9515\n",
      "Epoch 2/10\n",
      "25142/25142 [==============================] - 161s 6ms/step - loss: 0.0901 - accuracy: 0.9677 - val_loss: 0.1447 - val_accuracy: 0.9415\n",
      "Epoch 3/10\n",
      "25142/25142 [==============================] - 162s 6ms/step - loss: 0.0649 - accuracy: 0.9755 - val_loss: 0.1580 - val_accuracy: 0.9300\n",
      "Epoch 4/10\n",
      "25142/25142 [==============================] - 161s 6ms/step - loss: 0.0517 - accuracy: 0.9797 - val_loss: 0.1578 - val_accuracy: 0.9330\n",
      "Epoch 5/10\n",
      "25142/25142 [==============================] - 170s 7ms/step - loss: 0.0414 - accuracy: 0.9840 - val_loss: 0.1197 - val_accuracy: 0.9540\n",
      "Epoch 6/10\n",
      "25142/25142 [==============================] - 161s 6ms/step - loss: 0.0352 - accuracy: 0.9862 - val_loss: 0.1465 - val_accuracy: 0.9448\n",
      "Epoch 7/10\n",
      "25142/25142 [==============================] - 159s 6ms/step - loss: 0.0303 - accuracy: 0.9885 - val_loss: 0.1248 - val_accuracy: 0.9551\n",
      "Epoch 8/10\n",
      "25142/25142 [==============================] - 162s 6ms/step - loss: 0.0251 - accuracy: 0.9902 - val_loss: 0.1711 - val_accuracy: 0.9411\n",
      "Epoch 9/10\n",
      "25142/25142 [==============================] - 160s 6ms/step - loss: 0.0226 - accuracy: 0.9907 - val_loss: 0.1328 - val_accuracy: 0.9555\n",
      "Epoch 10/10\n",
      "25142/25142 [==============================] - 161s 6ms/step - loss: 0.0227 - accuracy: 0.9915 - val_loss: 0.1520 - val_accuracy: 0.9539\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "#callbacks = [EarlyStopping(monitor='val_loss', patience=4, verbose=1),]\n",
    "    \n",
    "history3 =model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13470/13470 [==============================] - 27s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "accr1 = model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AKURASI DARI LSTM \n",
      "Test set\n",
      "  Loss: 0.152\n",
      "  Accuracy: 0.956\n"
     ]
    }
   ],
   "source": [
    "print('AKURASI DARI LSTM \\nTest set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr1[0],accr1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda5f4ca4d05ec0428e858f0d437d648272"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
